{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have a llama pretraind model in /data/llm/llama/Llama-3.2-1B-Instruct, Load it and use it to generate text\n",
    "# I have a llama pretraind model in /data/llm/llama/Llama-3.2-1B-Instruct, Load it and use it to generate text\n",
    "CHECKPOINT_DIR=\"/data/llm/llama/Llama-3.2-1B-Instruct/original\"\n",
    "LLAMA_MODEL_MODULES_PATH=\"/raid/students/gerry/repos/llama-models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LLAMA_MODEL_MODULES_PATH to sys.path\n",
    "import sys\n",
    "if LLAMA_MODEL_MODULES_PATH not in sys.path:\n",
    "    sys.path.append(LLAMA_MODEL_MODULES_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/raid/students/gerry/repos/llama-models/models/llama3/api/tokenizer.model'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "SCRIPTS_DIR = Path(LLAMA_MODEL_MODULES_PATH)/\"models/scripts\"\n",
    "TOKENIZER_PATH = str(SCRIPTS_DIR.parent / \"llama3/api/tokenizer.model\")\n",
    "TOKENIZER_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['RANK'] = str(0)\n",
    "os.environ['WORLD_SIZE'] = str(1)\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12355'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import fire\n",
    "\n",
    "from models.llama3.api.datatypes import (\n",
    "    CompletionMessage,\n",
    "    StopReason,\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    ")\n",
    "\n",
    "from models.llama3.reference_impl.generation import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_llama(\n",
    "    ckpt_dir: str,\n",
    "    temperature: float = 0.6,\n",
    "    top_p: float = 0.9,\n",
    "    max_seq_len: int = 512,\n",
    "    max_batch_size: int = 4,\n",
    "    max_gen_len: Optional[int] = None,\n",
    "    model_parallel_size: Optional[int] = 1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Examples to run with the models finetuned for chat. Prompts correspond of chat\n",
    "    turns between the user and assistant with the final one always being the user.\n",
    "\n",
    "    An optional system prompt at the beginning to control how the model should respond\n",
    "    is also supported.\n",
    "\n",
    "    `max_gen_len` is optional because finetuned models are able to stop generations naturally.\n",
    "    \"\"\"\n",
    "    tokenizer_path = str(TOKENIZER_PATH)\n",
    "    generator = Llama.build(\n",
    "        ckpt_dir=ckpt_dir,\n",
    "        tokenizer_path=tokenizer_path,\n",
    "        max_seq_len=max_seq_len,\n",
    "        max_batch_size=max_batch_size,\n",
    "        model_parallel_size=model_parallel_size,\n",
    "    )\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/students/gerry/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/students/gerry/venv/lib/python3.8/site-packages/torch/__init__.py:955: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in 1.62 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create a gradio interface to interact with the model\n",
    "import gradio as gr\n",
    "\n",
    "def chat_completion(text):\n",
    "    llama = setup_llama(CHECKPOINT_DIR)\n",
    "\n",
    "    result = llama.chat_completion(\n",
    "        [UserMessage(content=text)],\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        max_gen_len=512,\n",
    "    )\n",
    "    #print(result.generation)\n",
    "    out_message = result.generation\n",
    "    return f\"> {out_message.role.capitalize()}: {out_message.content}\"\n",
    "\n",
    "def main():\n",
    "    gr.Interface(fn=chat_completion, \n",
    "                 inputs=\"text\",\n",
    "                 outputs=\"text\",\n",
    "                 title=\"Llama 3.2 Chat (AI 231) ðŸ¦™\",\n",
    "                 description=\"Chat with the Llama 3.2 model sample\",\n",
    "                 allow_flagging=\"never\").launch(inline=True, server_port=7864)\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
